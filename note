线下：0.0864748013952　线上: 0.103694
线下: 0.0864375482669 线上: 0.103447  num_leaves 200 learning_rate 0.1
线下: 0.0863518788431  线上:0.103548  num_leaves 256 learning_rate 0.1
线下:0.0864567665955                  num_leaves 210 learning_rate 0.1
线下：0.086333508534   线上：0.103319    num_leaves 200 learning_rate 0.05 min_child_sample 10
线下：0.0863152464051　线上：          num_leaves 200 learning_rate 0.05 min_child_samples 50
线下：0.0863471263107　线上：          num_leaves 200 learning_rate 0.05 min_child_sample 25
线下：0.0864034986609　线上：          num_leaves 200 learning_rate 0.05 min_child_samples 100

线下：0.0862394545981　线上：0.103386       num_leaves 200 learning_rate 0.03
线下：0.0862487311164  线上:           num_leaves 200 learning_rate 0.03 min_child_samples 50


to get the good results by leaf-wise tree, there are some important parameters:

1.num_leaves. This is the main parameter to control the complexity of tree model.
Theoretically, we can num_leaves = 2^(max_depth) to convert from depth-wise tree. However,
This simple conversion is not good in practice. The reason is, when number of leaves are the same, the leaf-wise tree is much deeper than depth-wise tree.
As a result, it may be over-fitting. Thus, when trying to tune the num_leaves, we should let it smaller than 2^(max_depth).
For example, when the max_depth=6 of depth-wise tree can get the good accuracy, set num_leaves to 127 may cause over-fitting, and set to 70 or 80 may get better accuracy than depth-wise.
Actually, the concept depth can be forgot in leaf-wise tree, since it doesn't have a correct mapping from leaves to depth.

2.min_data_in_leaf. This is a very important paramater to deal with over-fitting in leaf-wise tree. Its value depends on the number of training data and num_leaves. Set it to a large value can avoid grow too deeper tree, but may cause under-fitting. In practice, set it to hundreds or thousands is engouh for the large dataset.

3.max_depth. You also can use max_depth to limit the tree depth explicitly.




3. Bagging参数：bagging_fraction+bagging_freq（必须同时设置）、feature_fraction
bagging_fraction : random select part of data
baggging_fre : 0 means disable bagging,K means will perform bagging at every k iterator

4. min_data_in_leaf、min_sum_hessian_in_leaf

离散特征:
connectionType
telecomsOperator
advertiserID
appCategoryFirstClass
appCategorySecondClass
positionType
hometown_province
hometown_city
residence_province
residence_city
age_hot
gender
education
marriageStatus
haveBaby
click_hour
click_minute
click_second
install_or_not